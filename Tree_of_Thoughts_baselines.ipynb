{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "70xpFwRfqH6q"
      ],
      "authorship_tag": "ABX9TyMnSjwlrIsJPDG1de1C96ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weezymatt/tree-of-thoughts_demo/blob/main/Tree_of_Thoughts_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree of Thoughts\n",
        "\n",
        "Implementation: https://github.com/princeton-nlp/tree-of-thought-llm"
      ],
      "metadata": {
        "id": "ugrifYHMShVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "from google.colab import userdata\n",
        "from pprint import pprint\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "PiVMJj7Fb0py"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install asteval\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "Mk5OItsESaGn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import pandas as pd\n",
        "from asteval import Interpreter"
      ],
      "metadata": {
        "id": "NtAYcVtwVAuv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game24 Baselines — Standard Prompt"
      ],
      "metadata": {
        "id": "kpnrK9hkWlBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt-4'\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=model_name)"
      ],
      "metadata": {
        "id": "jgW7OWZuTdZN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-shot\n",
        "standard_prompt = '''Use numbers and basic arithmetic operations (+ - * /) to obtain 24.\n",
        "Input: 4 4 6 8\n",
        "Answer: (4 + 8) * (6 - 4) = 24\n",
        "Input: 2 9 10 12\n",
        "Answer: 2 * 12 * (10 - 9) = 24\n",
        "Input: 4 9 10 13\n",
        "Answer: (13 - 9) * (10 - 4) = 24\n",
        "Input: 1 4 8 8\n",
        "Answer: (8 / 4 + 1) * 8 = 24\n",
        "Input: 5 5 5 9\n",
        "Answer: 5 + 5 + 5 + 9 = 24\n",
        "Input: {input}\n",
        "'''"
      ],
      "metadata": {
        "id": "sRmt74gPejH2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game24Task:\n",
        "  \"\"\"\n",
        "  Game24 Class for experimenting with the various baselines in the paper.\n",
        "\n",
        "  Baselines     : standard prompting, chain-of-thought, and self-consistency (value-based)\n",
        "  \"\"\"\n",
        "  def __init__(self, path=\"https://hub.oxen.ai/api/repos/datasets/Game-of-24/file/main/24.csv\"):\n",
        "    self.data = pd.read_csv(path)\n",
        "    self.interpreter = Interpreter()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data.iloc[idx]\n",
        "\n",
        "  def standard_prompt(self, x):\n",
        "    return standard_prompt.format(input=x)\n",
        "\n",
        "  def cot_prompt(self, x):\n",
        "    return cot_prompt.format(input=x)\n",
        "\n",
        "  def parse_prompt(self, candidate_answer, prompt_type=\"standard_prompt\"):\n",
        "    answer = candidate_answer.content.lower()\n",
        "\n",
        "    if prompt_type == \"standard_prompt\":\n",
        "      answer = answer.split(\"answer: \")[-1].split(\" = \")[0]\n",
        "\n",
        "    if prompt_type == \"chain_of_thought\":\n",
        "      answer = answer.split(\"answer: \")[1].split(\" = \")[0]\n",
        "\n",
        "    return answer\n",
        "\n",
        "  def evaluate_expression(self, expression):\n",
        "    response = self.interpreter.eval(expression)\n",
        "\n",
        "    return {\n",
        "        \"expression\": expression,\n",
        "        \"answer\": response,\n",
        "        \"valid\": response == 24,\n",
        "    }\n",
        "\n",
        "game24 = Game24Task()"
      ],
      "metadata": {
        "id": "YmER46hSWO_7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game24 = Game24Task()\n",
        "x = game24.__getitem__(10)['Puzzles']\n",
        "standard_prompt = game24.standard_prompt(x)"
      ],
      "metadata": {
        "id": "EiTK7MLvf9cC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(standard_prompt)\n",
        "expression = game24.parse_prompt(output)"
      ],
      "metadata": {
        "id": "DXV86fpUh_DI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(game24.evaluate_expression(expression), width=50, sort_dicts=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy_5gqJFz8WD",
        "outputId": "1bfd7160-8a24-4180-a2c8-533711bf484b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'expression': '(8 - 1) * (2 + 1)',\n",
            " 'answer': 21,\n",
            " 'valid': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game24 Baselines - Chain-of-Thought Prompt"
      ],
      "metadata": {
        "id": "70xpFwRfqH6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-shot\n",
        "cot_prompt = '''Use numbers and basic arithmetic operations (+ - * /) to obtain 24. Each step, you are only allowed to choose two of the remaining numbers to obtain a new number.\n",
        "Input: 4 4 6 8\n",
        "Steps:\n",
        "4 + 8 = 12 (left: 4 6 12)\n",
        "6 - 4 = 2 (left: 2 12)\n",
        "2 * 12 = 24 (left: 24)\n",
        "Answer: (6 - 4) * (4 + 8) = 24\n",
        "Input: 2 9 10 12\n",
        "Steps:\n",
        "12 * 2 = 24 (left: 9 10 24)\n",
        "10 - 9 = 1 (left: 1 24)\n",
        "24 * 1 = 24 (left: 24)\n",
        "Answer: (12 * 2) * (10 - 9) = 24\n",
        "Input: 4 9 10 13\n",
        "Steps:\n",
        "13 - 10 = 3 (left: 3 4 9)\n",
        "9 - 3 = 6 (left: 4 6)\n",
        "4 * 6 = 24 (left: 24)\n",
        "Answer: 4 * (9 - (13 - 10)) = 24\n",
        "Input: 1 4 8 8\n",
        "Steps:\n",
        "8 / 4 = 2 (left: 1 2 8)\n",
        "1 + 2 = 3 (left: 3 8)\n",
        "3 * 8 = 24 (left: 24)\n",
        "Answer: (1 + 8 / 4) * 8 = 24\n",
        "Input: 5 5 5 9\n",
        "Steps:\n",
        "5 + 5 = 10 (left: 5 9 10)\n",
        "10 + 5 = 15 (left: 9 15)\n",
        "15 + 9 = 24 (left: 24)\n",
        "Answer: ((5 + 5) + 5) + 9 = 24\n",
        "Input: {input}\n",
        "'''"
      ],
      "metadata": {
        "id": "8QHDUsOYgf61"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = game24.__getitem__(10)['Puzzles']\n",
        "cot_prompt = game24.cot_prompt(x)"
      ],
      "metadata": {
        "id": "t_zWdZ76_dA8"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(cot_prompt)"
      ],
      "metadata": {
        "id": "bVnPH0rbvDsP"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expression = game24.parse_prompt(output, prompt_type=\"chain_of_thought\")\n",
        "pprint(game24.evaluate_expression(expression), width=50, sort_dicts=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llAiUNeyOT8s",
        "outputId": "272b4fe8-e073-4731-bf82-db18124b7363"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'expression': '(1 + 1 + 1) * 8',\n",
            " 'answer': 24,\n",
            " 'valid': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebXw8NxTTsW",
        "outputId": "94b176e6-8379-4919-c662-5672b80e065f"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps:\n",
            "1 + 1 = 2 (left: 1 2 8)\n",
            "2 + 1 = 3 (left: 3 8)\n",
            "3 * 8 = 24 (left: 24)\n",
            "Answer: (1 + 1 + 1) * 8 = 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game24 Baselines - Chain-of-Thought + Self-Consistency\n",
        "\"We also consider a CoT self-consistency baseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on\n",
        "top of an IO sample for at most 10 iterations. At each iteration, the LM is conditioned on all previous history to “reflect on your mistakes and generate a refined answer” if the output is incorrect. Note that it uses groundtruth feedback signals about equation correctness.\""
      ],
      "metadata": {
        "id": "GJeNJk7QrAJI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BMTrBu_ZaT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}