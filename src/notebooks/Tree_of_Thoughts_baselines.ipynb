{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbDHFBBP1dy6RMCiJO4cYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weezymatt/tree-of-thoughts_demo/blob/main/src/notebooks/Tree_of_Thoughts_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree of Thoughts\n",
        "\n",
        "Implementation: https://github.com/princeton-nlp/tree-of-thought-llm"
      ],
      "metadata": {
        "id": "ugrifYHMShVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "from google.colab import userdata\n",
        "from pprint import pprint\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "PiVMJj7Fb0py"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install asteval\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "Mk5OItsESaGn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import pandas as pd\n",
        "from asteval import Interpreter"
      ],
      "metadata": {
        "id": "NtAYcVtwVAuv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game24 Baselines — Standard Prompt"
      ],
      "metadata": {
        "id": "kpnrK9hkWlBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt-4'\n",
        "llm = ChatOpenAI(temperature=0.7, model_name=model_name)"
      ],
      "metadata": {
        "id": "jgW7OWZuTdZN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game24Task:\n",
        "  \"\"\"\n",
        "  Game24 Class for experimenting with the various baselines in the paper.\n",
        "\n",
        "  Baselines     : standard prompting, chain-of-thought, and self-consistency (value-based)\n",
        "  \"\"\"\n",
        "  def __init__(self, path=\"https://hub.oxen.ai/api/repos/datasets/Game-of-24/file/main/24.csv\"):\n",
        "    self.data = pd.read_csv(path)\n",
        "    self.interpreter = Interpreter()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data.iloc[idx]\n",
        "\n",
        "  def sample(self, n=None):\n",
        "    return self.data.sample(n=n)\n",
        "\n",
        "  def standard_prompt(self, x):\n",
        "    return standard_prompt.format(input=x)\n",
        "\n",
        "  def cot_prompt(self, x):\n",
        "    return cot_prompt.format(input=x)\n",
        "\n",
        "  def parse_prompt(self, candidate_answer, prompt_type=\"standard_prompt\"):\n",
        "    errors = 0\n",
        "    answer = candidate_answer.content.lower()\n",
        "\n",
        "    if prompt_type == \"standard_prompt\":\n",
        "      answer = answer.split(\"answer: \")[-1].split(\" = \")[0]\n",
        "\n",
        "    if prompt_type == \"chain_of_thought\":\n",
        "      try:\n",
        "        answer = answer.split(\"answer: \")[1].split(\" = \")[0]\n",
        "      except:\n",
        "        errors += 1\n",
        "        print(\"Error.\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "  def evaluate_expression(self, expression, puzzle):\n",
        "    try:\n",
        "      response = self.interpreter.eval(expression)\n",
        "    except:\n",
        "      response = None\n",
        "\n",
        "    return {\n",
        "        \"puzzle\": puzzle,\n",
        "        \"expression\": expression,\n",
        "        \"answer\": response,\n",
        "        \"valid\": response == 24,\n",
        "    }"
      ],
      "metadata": {
        "id": "YmER46hSWO_7"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-shot\n",
        "standard_prompt = '''Use numbers and basic arithmetic operations (+ - * /) to obtain 24.\n",
        "Input: 4 4 6 8\n",
        "Answer: (4 + 8) * (6 - 4) = 24\n",
        "Input: 2 9 10 12\n",
        "Answer: 2 * 12 * (10 - 9) = 24\n",
        "Input: 4 9 10 13\n",
        "Answer: (13 - 9) * (10 - 4) = 24\n",
        "Input: 1 4 8 8\n",
        "Answer: (8 / 4 + 1) * 8 = 24\n",
        "Input: 5 5 5 9\n",
        "Answer: 5 + 5 + 5 + 9 = 24\n",
        "Input: {input}\n",
        "'''"
      ],
      "metadata": {
        "id": "UoD3bFem-XB7"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple usage\n",
        "game24 = Game24Task()\n",
        "x = game24.__getitem__(10)['Puzzles']\n",
        "standard_prompt = game24.standard_prompt(x)"
      ],
      "metadata": {
        "id": "EiTK7MLvf9cC"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(standard_prompt)\n",
        "expression = game24.parse_prompt(output)"
      ],
      "metadata": {
        "id": "DXV86fpUh_DI"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(game24.evaluate_expression(expression, x), width=50, sort_dicts=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy_5gqJFz8WD",
        "outputId": "7fea6410-9107-4af0-a090-11adb45738a9"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'puzzle': '1 1 2 8',\n",
            " 'expression': '8 * (2 - (1 / 1))',\n",
            " 'answer': 8.0,\n",
            " 'valid': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game24 Baselines - Chain-of-Thought Prompt"
      ],
      "metadata": {
        "id": "70xpFwRfqH6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-shot\n",
        "cot_prompt = '''Use numbers and basic arithmetic operations (+ - * /) to obtain 24. Each step, you are only allowed to choose two of the remaining numbers to obtain a new number.\n",
        "Input: 4 4 6 8\n",
        "Steps:\n",
        "4 + 8 = 12 (left: 4 6 12)\n",
        "6 - 4 = 2 (left: 2 12)\n",
        "2 * 12 = 24 (left: 24)\n",
        "Answer: (6 - 4) * (4 + 8) = 24\n",
        "Input: 2 9 10 12\n",
        "Steps:\n",
        "12 * 2 = 24 (left: 9 10 24)\n",
        "10 - 9 = 1 (left: 1 24)\n",
        "24 * 1 = 24 (left: 24)\n",
        "Answer: (12 * 2) * (10 - 9) = 24\n",
        "Input: 4 9 10 13\n",
        "Steps:\n",
        "13 - 10 = 3 (left: 3 4 9)\n",
        "9 - 3 = 6 (left: 4 6)\n",
        "4 * 6 = 24 (left: 24)\n",
        "Answer: 4 * (9 - (13 - 10)) = 24\n",
        "Input: 1 4 8 8\n",
        "Steps:\n",
        "8 / 4 = 2 (left: 1 2 8)\n",
        "1 + 2 = 3 (left: 3 8)\n",
        "3 * 8 = 24 (left: 24)\n",
        "Answer: (1 + 8 / 4) * 8 = 24\n",
        "Input: 5 5 5 9\n",
        "Steps:\n",
        "5 + 5 = 10 (left: 5 9 10)\n",
        "10 + 5 = 15 (left: 9 15)\n",
        "15 + 9 = 24 (left: 24)\n",
        "Answer: ((5 + 5) + 5) + 9 = 24\n",
        "Input: {input}\n",
        "'''"
      ],
      "metadata": {
        "id": "8QHDUsOYgf61"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple usage\n",
        "game24 = Game24Task()\n",
        "x = game24.__getitem__(10)['Puzzles']\n",
        "cot_prompt = game24.cot_prompt(x)"
      ],
      "metadata": {
        "id": "t_zWdZ76_dA8"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(cot_prompt)"
      ],
      "metadata": {
        "id": "bVnPH0rbvDsP"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expression = game24.parse_prompt(output, prompt_type=\"chain_of_thought\")\n",
        "pprint(game24.evaluate_expression(expression, x), width=50, sort_dicts=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llAiUNeyOT8s",
        "outputId": "85e3a883-7b5f-4534-d975-efc2bf257861"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'puzzle': '1 1 2 8',\n",
            " 'expression': '8 * 2 + 1 + 1',\n",
            " 'answer': 18,\n",
            " 'valid': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game24 Experiments - (Standard Prompt, CoT)"
      ],
      "metadata": {
        "id": "rbvDoiUbY9Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(experiment_type=\"io\", n_samples=100):\n",
        "  prompts = []\n",
        "  outputs = []\n",
        "  expressions = []\n",
        "  evaluations = []\n",
        "  prediction = []\n",
        "\n",
        "  samples = game24.sample(n=n_samples)['Puzzles']\n",
        "\n",
        "  for sample in samples:\n",
        "    if experiment_type == \"io\":\n",
        "      fitted_prompt = game24.standard_prompt(sample)\n",
        "    if experiment_type == \"cot\":\n",
        "      fitted_prompt = game24.cot_prompt(sample)\n",
        "    prompts.append(fitted_prompt)\n",
        "\n",
        "  for prompt in prompts:\n",
        "    output = llm.invoke(prompt)\n",
        "    outputs.append(output)\n",
        "\n",
        "  try:\n",
        "    for output, sample in zip(outputs, samples):\n",
        "      if experiment_type == \"io\":\n",
        "        parsed = game24.parse_prompt(prompt_type=\"standard_prompt\")\n",
        "      if experiment_type == \"cot\":\n",
        "        parsed = game24.parse_prompt(prompt_type=\"cot\")\n",
        "\n",
        "      evals = game24.evaluate_expression(parsed, sample)\n",
        "      expressions.append(parsed)\n",
        "      evaluations.append(evals)\n",
        "\n",
        "  except:\n",
        "    print(\"Expression unable to evaluate.\")\n",
        "\n",
        "  for sample in evaluations:\n",
        "    prediction.append(sample['valid'])\n",
        "\n",
        "  return {\n",
        "      \"samples\": samples,\n",
        "      \"prompts\": prompts,\n",
        "      \"outputs\": outputs,\n",
        "      \"expressions\": expressions,\n",
        "      \"evaluations\": evaluations,\n",
        "      \"predictions\": predictions,\n",
        "  }"
      ],
      "metadata": {
        "id": "0Z9YnwanZItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataframe(samples_series, expressions, score):\n",
        "  samples_df = samples_series.to_frame()\n",
        "  results = samples_df.assign(expression=expressions, score=score)\n",
        "  return results\n",
        "\n",
        "final_results = create_dataframe(samples, expressions, predictions)\n",
        "# final_results.to_csv('string.csv', index=False)"
      ],
      "metadata": {
        "id": "1Ovtad7dZMlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results on experiment\n",
        "\n",
        "| Method     | Success   | Runs |\n",
        "| --------   | --------  | ---- |\n",
        "| IO prompt  | 10%       | 1    |\n",
        "| CoT prompt | 26%       | 1    |\n",
        "| ToT (b=?)  | ??%       | 1    |"
      ],
      "metadata": {
        "id": "G-QcpKq-ZtCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game24 Baselines - Chain-of-Thought + Self-Consistency (not implemented)\n",
        "\"We also consider a CoT self-consistency baseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on\n",
        "top of an IO sample for at most 10 iterations. At each iteration, the LM is conditioned on all previous history to “reflect on your mistakes and generate a refined answer” if the output is incorrect. Note that it uses groundtruth feedback signals about equation correctness.\""
      ],
      "metadata": {
        "id": "GJeNJk7QrAJI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BMTrBu_ZaT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}