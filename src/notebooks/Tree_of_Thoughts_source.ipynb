{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1sy4Dfjhgz-"
   },
   "source": [
    "# Tree of Thoughts\n",
    "\n",
    "Implementation: https://github.com/princeton-nlp/tree-of-thought-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from asteval import Interpreter\n",
    "\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB0jcYL9hsAX"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tot.methods.bfs import solve\n",
    "from tot.tasks.game24 import Game24Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcMjfX7Khxra"
   },
   "outputs": [],
   "source": [
    "task = Game24Task()\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game24Data:\n",
    "    \"\"\"\n",
    "    Game24 class for experimenting with tree of thought.\n",
    "    \"\"\"\n",
    "    def __init__(self, path=\"https://hub.oxen.ai/api/repos/datasets/Game-of-24/file/main/24.csv\"):\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.interpreter = Interpreter()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]\n",
    "    \n",
    "    def sample(self, n=None):\n",
    "        return self.data.sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beams = 1 # per paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnORUmbWh5F3"
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(backend='gpt-4',\n",
    "                          temperature=0.7,\n",
    "                          task='game24',\n",
    "                          naive_run=False,\n",
    "                          prompt_sample=None,\n",
    "                          method_generate='propose',\n",
    "                          method_evaluate='value',\n",
    "                          method_select='greedy',\n",
    "                          n_generate_sample=1,\n",
    "                          n_evaluate_sample=3,\n",
    "                          n_select_sample=beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from repository\n",
    "task = Game24Task()\n",
    "ys, infos = solve(args, task, 900)\n",
    "print(ys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 â€” 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game24_data = Game24Data()\n",
    "samples = game24_data.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source to silence stdout: https://stackoverflow.com/questions/2828953/silence-the-stdout-of-a-function-in-python-without-trashing-sys-stdout-and-resto\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def silence_stdout():\n",
    "    save_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    yield\n",
    "    sys.stdout = save_stdout\n",
    "\n",
    "def run_experiment():\n",
    "    infos_list = []\n",
    "    ys_list = []\n",
    "    \n",
    "    for sample_idx in samples.index:\n",
    "        ys, infos = solve(args, task, sample_idx)\n",
    "        infos_list.append(infos)\n",
    "        ys_list.append(ys)\n",
    "    \n",
    "    return ys_list, infos_list\n",
    "    \n",
    "        \n",
    "# with silence_stdout():\n",
    "#     start = time.time()\n",
    "#     ys_list, infos_list = run_experiment() #prevent stdout... too much!\n",
    "#     end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "# infos_results = \"../../logs/game24/gpt-4_propose1_value1_greedy_samples100.json\"\n",
    "# json_string = json.dumps(infos_list, indent=4)\n",
    "\n",
    "# with open(infos_results, \"w\") as file:\n",
    "#     file.write(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on experiment 2\n",
    "Run time: 60 minutes (3658.445431947708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_results = \"../../logs/game24/gpt-4_propose1_value1_greedy_samples100.json\"\n",
    "\n",
    "with open(infos_results) as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tot():\n",
    "    expressions = {}\n",
    "    evaluations = []\n",
    "    \n",
    "    for idx, _ in enumerate(data):\n",
    "        result = data[idx]['steps'][3]['select_new_ys'][0].split(\"Answer: \")[-1].split(\"= 24\")[0]\n",
    "        expressions[idx] = result\n",
    "    \n",
    "    with silence_stdout():\n",
    "        for k, v in expressions.items():\n",
    "            try:\n",
    "                evaluations.append(24==game24_data.interpreter.eval(v))\n",
    "            except:\n",
    "                evaluations.append(False)\n",
    "\n",
    "    return expressions, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps, acc = evaluate_tot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method     | Success   | Runs |\n",
    "| --------   | --------  | ---- |\n",
    "| IO prompt  | 10%       | 1    |\n",
    "| CoT prompt | 26%       | 1    |\n",
    "| ToT (b=1)  | 50%       | 1    |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPALrP/gx4J/7iRGdRDp+wJ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tree of thoughts",
   "language": "python",
   "name": "tot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
